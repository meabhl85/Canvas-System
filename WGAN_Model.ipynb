{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP/zUQhwYR0YT2I2GfWk50n"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVVgDEzw8p8k"
      },
      "source": [
        "#### Imports and Google Drive Connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ozc5ix4J1O7x",
        "outputId": "02f2d9c5-4a1c-469b-ade7-db023bbec34e"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Reshape, Dropout, Dense \n",
        "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
        "from tensorflow.keras.layers import Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.constraints import Constraint\n",
        "from tensorflow.keras import backend\n",
        "import numpy as np\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from numpy import expand_dims\n",
        "from numpy import mean\n",
        "from numpy import ones\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import os \n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False\n",
        "\n",
        "#Nicely formatted time string\n",
        "def hms_string(sec_elapsed):\n",
        "  h = int(sec_elapsed / (60 * 60))\n",
        "  m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "  s = sec_elapsed % 60\n",
        "  return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Note: using Google CoLab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUgQ-ZxG8yVk"
      },
      "source": [
        "#### Model Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSNPkHg_1Tku"
      },
      "source": [
        "#Image Sizes\n",
        "GENERATE_RES = 5 # Generation resolution factor \n",
        "GENERATE_SQUARE = 32 * GENERATE_RES # rows/cols (should be square)\n",
        "IMAGE_CHANNELS = 3\n",
        "\n",
        "#Input and Output Paths\n",
        "NPY_FILES_PATH = '/content/drive/MyDrive/Npy_Files/'\n",
        "OUTPUT_IMAGE_PATH = '/content/drive/MyDrive/Colab Notebooks/honours_project/GAN_Images/'\n",
        "\n",
        "# Preview image \n",
        "PREVIEW_ROWS = 2\n",
        "PREVIEW_COLS = 3\n",
        "PREVIEW_MARGIN = 16\n",
        "\n",
        "# Size vector to generate images from\n",
        "SEED_SIZE = 800\n",
        "EPOCHS = 1000\n",
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 60000\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JrV4xkU856j"
      },
      "source": [
        "#### Additional Methods for WGAN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUIoKK1X1VhT"
      },
      "source": [
        "# clip model weights to a given hypercube\n",
        "class ClipConstraint(Constraint):\n",
        "\t# set clip value when initialized\n",
        "\tdef __init__(self, clip_value):\n",
        "\t\tself.clip_value = clip_value\n",
        " \n",
        "\t# clip model weights to hypercube\n",
        "\tdef __call__(self, weights):\n",
        "\t\treturn backend.clip(weights, -self.clip_value, self.clip_value)\n",
        " \n",
        "\t# get the config\n",
        "\tdef get_config(self):\n",
        "\t\treturn {'clip_value': self.clip_value}\n",
        " \n",
        "# calculate wasserstein loss\n",
        "def wasserstein_loss(y_true, y_pred):\n",
        "\treturn backend.mean(y_true * y_pred)\n",
        " "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkaA_JEB9ANH"
      },
      "source": [
        "#### Model Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgkxAr_x1cRJ"
      },
      "source": [
        "class WGAN:\n",
        "  def __init__(self, input_data_name):\n",
        "    self.data_name = input_data_name\n",
        "    self.image_shape = (GENERATE_SQUARE, GENERATE_SQUARE, IMAGE_CHANNELS)\n",
        "    self.training_binary_path = os.path.join(NPY_FILES_PATH, self.data_name + f'_training_data_{GENERATE_SQUARE}_{GENERATE_SQUARE}.npy')\n",
        "    self.loss_function = tf.keras.losses.BinaryCrossentropy()\n",
        "    self.output_image_paths = self.data_name\n",
        "\n",
        "    self.output_image_path = os.path.join(OUTPUT_IMAGE_PATH, os.path.join(self.data_name, f'images_square_{GENERATE_SQUARE}_res_{GENERATE_RES}_seed_{SEED_SIZE}_epochs_{EPOCHS}_batchSize_{BATCH_SIZE}_bufferSize_{BUFFER_SIZE}.npy'))\n",
        "    if not os.path.exists(self.output_image_path):\n",
        "      os.makedirs(self.output_image_path)\n",
        "\n",
        "    print(f\"Looking for file: {self.training_binary_path}\")\n",
        "    if not os.path.isfile(self.training_binary_path):\n",
        "      print('No NPY file...')\n",
        "    else:\n",
        "      print(\"Loading previous training pickle...\")\n",
        "      training_data = np.load(self.training_binary_path)\n",
        "      \n",
        "      #Batch and Shuffle the Data\n",
        "      self.training_dataset = tf.data.Dataset.from_tensor_slices(training_data).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)  \n",
        "    \n",
        "  def build_generator(self):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(4*4*256, activation = \"relu\", input_dim = SEED_SIZE))\n",
        "    model.add(Reshape((4, 4, 256)))\n",
        "\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(256, kernel_size = 3, padding = \"same\"))\n",
        "    model.add(BatchNormalization(momentum = 0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(256, kernel_size = 3, padding = \"same\"))\n",
        "    model.add(BatchNormalization(momentum = 0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "   \n",
        "    # Output resolution, additional upsampling\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(128, kernel_size = 3, padding = \"same\"))\n",
        "    model.add(BatchNormalization(momentum = 0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    if GENERATE_RES > 1:\n",
        "      model.add(UpSampling2D(size = (GENERATE_RES, GENERATE_RES)))\n",
        "      model.add(Conv2D(128,kernel_size=3, padding=\"same\"))\n",
        "      model.add(BatchNormalization(momentum = 0.8))\n",
        "      model.add(Activation(\"relu\"))\n",
        "\n",
        "    # Final CNN layer\n",
        "    model.add(Conv2D(IMAGE_CHANNELS, kernel_size=3, padding=\"same\"))\n",
        "    model.add(Activation(\"tanh\"))\n",
        "\n",
        "    return model\n",
        "\n",
        "  def build_discriminator(self):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(32, kernel_size = 3, strides = 2, input_shape = self.image_shape, padding = \"same\"))\n",
        "    model.add(LeakyReLU(alpha = 0.2))\n",
        "\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(64, kernel_size = 3, strides = 2, padding = \"same\"))\n",
        "    model.add(ZeroPadding2D(padding = ((0, 1), (0, 1))))\n",
        "    model.add(BatchNormalization(momentum = 0.8))\n",
        "    model.add(LeakyReLU(alpha = 0.2))\n",
        "\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(128, kernel_size = 3, strides = 2, padding = \"same\"))\n",
        "    model.add(BatchNormalization(momentum = 0.8))\n",
        "    model.add(LeakyReLU(alpha = 0.2))\n",
        "\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(256, kernel_size = 3, strides = 1, padding = \"same\"))\n",
        "    model.add(BatchNormalization(momentum = 0.8))\n",
        "    model.add(LeakyReLU(alpha = 0.2))\n",
        "\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Conv2D(512, kernel_size = 3, strides = 1, padding = \"same\"))\n",
        "    model.add(BatchNormalization(momentum = 0.8))\n",
        "    model.add(LeakyReLU(alpha = 0.2))\n",
        "\n",
        "    model.add(Dropout(0.25))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # compile model\n",
        "    opt = RMSprop(lr=0.0005)\n",
        "    model.compile(loss=wasserstein_loss, optimizer=opt)\n",
        "\n",
        "    return model\n",
        " \n",
        "  def build_combined(self):\n",
        "  \n",
        "    for layer in self.discriminator.layers:\n",
        "      if not isinstance(layer, BatchNormalization):\n",
        "        layer.trainable = False\n",
        "  \n",
        "    model = Sequential()\n",
        "    model.add(self.generator)\n",
        "    model.add(self.discriminator)\n",
        "\n",
        "    opt = RMSprop(lr=0.0005)\n",
        "\n",
        "    model.compile(loss=wasserstein_loss, optimizer=opt)\n",
        "    \n",
        "    return model\n",
        "\n",
        "  def generator_loss(self, fake_pred):\n",
        "    #As G wants to create real images the loss is cross entropy of 1's and the generated output\n",
        "    return self.loss_function(tf.ones_like(fake_pred), fake_pred)\n",
        "\n",
        "  def discriminator_loss(self, real_pred, fake_pred):\n",
        "    #D should compare the real predictions to 1 and fake to 0 (as they are the real values)\n",
        "    real_image_loss = self.loss_function(tf.ones_like(real_pred), real_pred)\n",
        "    fake_image_loss = self.loss_function(tf.zeros_like(fake_pred), fake_pred)\n",
        "\n",
        "    #Discriminator uses the complete loss\n",
        "    final_loss = real_image_loss + fake_image_loss\n",
        "    return final_loss\n",
        "\n",
        "  def compile_models(self):\n",
        "    self.generator_optimiser = tf.keras.optimizers.Adam(1.2e-4, 0.4)\n",
        "    self.discriminator_optimiser = tf.keras.optimizers.Adam(1.2e-4, 0.4)\n",
        "\n",
        "  @tf.function\n",
        "  def generator_train(self, real_images):\n",
        "    seed = tf.random.normal([BATCH_SIZE, SEED_SIZE])\n",
        "\n",
        "    #Performs automatic differentiation used for backpropagation \n",
        "    with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
        "      #Gnerate images with noise vector\n",
        "      generated_images = self.generator(seed, training = True)\n",
        "\n",
        "      #Put resulting image into the D and return prediction\n",
        "      fake_pred = self.discriminator(generated_images, training = True)\n",
        "\n",
        "      #Calculate loss for both G\n",
        "      g_loss = self.generator_loss(fake_pred)\n",
        "      wasserstein_loss(real_pred, fake_pred)\n",
        "\n",
        "    #Calculate gradients\n",
        "    gen_gradients = g_tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "\n",
        "    #Update weights \n",
        "    self.generator_optimiser.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))\n",
        "\n",
        "    return g_loss\n",
        "\n",
        "  @tf.function\n",
        "  def discriminator_train(self, real_images):\n",
        "    seed = tf.random.normal([BATCH_SIZE, SEED_SIZE])\n",
        "\n",
        "    #Performs automatic differentiation used for backpropagation \n",
        "    with tf.GradientTape() as d_tape:\n",
        "      #Gnerate images with noise vector\n",
        "      generated_images = self.generator(seed, training = False)\n",
        "\n",
        "      #Put resulting image into the D and return prediction\n",
        "      real_pred = self.discriminator(real_images, training = True)\n",
        "      fake_pred = self.discriminator(generated_images, training = True)\n",
        "\n",
        "      real_y = -ones((real_images.shape[0], 1))\n",
        "\n",
        "      #Calculate loss for D\n",
        "      d_loss = wasserstein_loss(real_pred, real_y)\n",
        "      print(d_loss)\n",
        "\n",
        "    #Calculate gradients\n",
        "    dis_gradients = d_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "\n",
        "    #Update weights \n",
        "    self.discriminator_optimiser.apply_gradients(zip(dis_gradients, self.discriminator.trainable_variables))\n",
        "\n",
        "    return d_loss\n",
        "\n",
        "  def train(self, data, epochs, save_epoch, save_checkpoint):\n",
        "    fixed_seed = np.random.normal(0, 1, (PREVIEW_ROWS*PREVIEW_COLS, SEED_SIZE))\n",
        "    start = time.time()\n",
        "    save_count = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      #Initalise timing and loss lists\n",
        "      epoch_time = time.time()\n",
        "      g_loss_list = []\n",
        "      d_loss1_list = []\n",
        "      d_loss2_list = []\n",
        "\n",
        "      #Run the step training\n",
        "      for image_batch in data:\n",
        "        seed = tf.random.normal([BATCH_SIZE, SEED_SIZE])\n",
        "        d1, d2 = list(), list()\n",
        "        for i in range(5):\n",
        "\n",
        "            real_y = -ones((image_batch.shape[0], 1))\n",
        "\n",
        "            # update critic model weights\n",
        "            c_loss1 = self.discriminator.train_on_batch(image_batch, real_y)\n",
        "            d1.append(c_loss1)\n",
        "            \n",
        "            # generate 'fake' examples\n",
        "            X_fake = self.generator(seed)\n",
        "\n",
        "            fake_y = -ones((X_fake.shape[0], 1))\n",
        "\n",
        "            # update critic model weights\n",
        "            c_loss2 = self.discriminator.train_on_batch(X_fake, fake_y)\n",
        "            d2.append(c_loss2)\n",
        "\n",
        "        d_loss1_list.append(mean(d1))\n",
        "        d_loss2_list.append(mean(d2))\n",
        "\n",
        "        \n",
        "        # create inverted labels for the fake samples\n",
        "        y_gan = -ones((seed.shape[0], 1))\n",
        "\n",
        "        # update the generator via the critic's error\n",
        "        g_loss = self.combined.train_on_batch(seed, y_gan)\n",
        "        \n",
        "        g_loss_list.append(g_loss)\n",
        "\n",
        "      #Average losses for entire image batch\n",
        "      average_g_loss = sum(g_loss_list) / len(g_loss_list)\n",
        "      average_d1_loss = sum(d_loss1_list) / len(d_loss1_list)\n",
        "      average_d2_loss = sum(d_loss2_list) / len(d_loss2_list)\n",
        "\n",
        "      #Calculate epoch time\n",
        "      epoch_finished = time.time() - epoch_time\n",
        "\n",
        "      print (f'Epoch {epoch + 1}, gen loss={average_g_loss}, disc loss={average_d1_loss}, disc loss={average_d2_loss}, epoch time={hms_string(epoch_finished)}')\n",
        "      \n",
        "      if save_count == save_epoch:\n",
        "        self.save_images(epoch, fixed_seed)\n",
        "        save_count = 0\n",
        "      save_count += 1\n",
        "\n",
        "      if epoch % save_checkpoint == 0:\n",
        "        self.save_models()\n",
        "\n",
        "    final_time = time.time()-start\n",
        "    print (f'Training time: {hms_string(final_time)}')\n",
        "    plot_history(d_loss1_list, d_loss2_list, g_loss_list)\n",
        "\n",
        "  def plot_history(self, d1_hist, d2_hist, g_hist):\n",
        "    # plot history\n",
        "    pyplot.plot(d1_hist, label='crit_real')\n",
        "    pyplot.plot(d2_hist, label='crit_fake')\n",
        "    pyplot.plot(g_hist, label='gen')\n",
        "    pyplot.legend()\n",
        "    pyplot.savefig('plot_line_plot_loss.png')\n",
        "    pyplot.close()\n",
        "\n",
        "  def save_models(self):\n",
        "    output_model_path = os.path.join(OUTPUT_IMAGE_PATH, self.data_name + \"_Models\")\n",
        "    if not os.path.exists(output_model_path):\n",
        "      os.makedirs(output_model_path)\n",
        "\n",
        "    self.generator.save(os.path.join(output_model_path, f'generator_model_{GENERATE_SQUARE}_res_{GENERATE_RES}_seed_{SEED_SIZE}_epochs_{EPOCHS}_batchSize_{BATCH_SIZE}_bufferSize_{BUFFER_SIZE}.npy'))\n",
        "    self.discriminator.save(os.path.join(output_model_path, f'discriminator_model_{GENERATE_SQUARE}_res_{GENERATE_RES}_seed_{SEED_SIZE}_epochs_{EPOCHS}_batchSize_{BATCH_SIZE}_bufferSize_{BUFFER_SIZE}.npy'))\n",
        "\n",
        "  def load_models(self):\n",
        "    output_model_path = os.path.join(OUTPUT_IMAGE_PATH, self.data_name + \"_Models\")\n",
        "    self.generator = load_model(os.path.join(output_model_path, f'generator_model_{GENERATE_SQUARE}_res_{GENERATE_RES}_seed_{SEED_SIZE}_epochs_{EPOCHS}_batchSize_{BATCH_SIZE}_bufferSize_{BUFFER_SIZE}.npy'))\n",
        "    self.discriminator = load_model(os.path.join(output_model_path, f'discriminator_model_{GENERATE_SQUARE}_res_{GENERATE_RES}_seed_{SEED_SIZE}_epochs_{EPOCHS}_batchSize_{BATCH_SIZE}_bufferSize_{BUFFER_SIZE}.npy'))\n",
        "\n",
        "  def save_images(self, count, noise):\n",
        "    image_array = np.full(( \n",
        "        PREVIEW_MARGIN + (PREVIEW_ROWS * (GENERATE_SQUARE+PREVIEW_MARGIN)), \n",
        "        PREVIEW_MARGIN + (PREVIEW_COLS * (GENERATE_SQUARE+PREVIEW_MARGIN)), 3), \n",
        "        255, dtype=np.uint8)\n",
        "    \n",
        "    generated_images = self.generator.predict(noise)\n",
        "\n",
        "    generated_images = 0.5 * generated_images + 0.5\n",
        "\n",
        "    image_count = 0\n",
        "    for row in range(PREVIEW_ROWS):\n",
        "        for col in range(PREVIEW_COLS):\n",
        "          r = row * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n",
        "          c = col * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n",
        "          image_array[r:r+GENERATE_SQUARE,c:c+GENERATE_SQUARE] \\\n",
        "              = generated_images[image_count] * 255\n",
        "          image_count += 1\n",
        "\n",
        "    filename = os.path.join(self.output_image_path, f\"train-{count}.png\")\n",
        "    im = Image.fromarray(image_array)\n",
        "    im.save(filename)\n",
        "\n",
        "  def show_image(self, data):\n",
        "    plt.figure()\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(data[0])\n",
        "    plt.show()\n",
        "\n",
        "  def run(self, new_model, save_epoch, save_checkpoint):\n",
        "\n",
        "    if new_model:\n",
        "      #Build generator and discriminator \n",
        "      self.generator = self.build_generator()\n",
        "      self.discriminator = self.build_discriminator()\n",
        "      self.combined = self.build_combined()\n",
        "    else:\n",
        "      #Load previous model\n",
        "      self.load_models()\n",
        "\n",
        "    self.compile_models()\n",
        "\n",
        "    print('Starting training...')\n",
        "    self.train(self.training_dataset, EPOCHS, save_epoch, save_checkpoint)\n",
        "\n",
        "    print('Saving generator model...')\n",
        "    self.save_models()\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMqkqs4u9Fmj"
      },
      "source": [
        "#### Running the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUFQ5STK2l0K",
        "outputId": "1b4a457a-571f-4ab9-d00c-60c611a77398"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  gan = WGAN('Impressionism')\n",
        "  gan.run(True, 1, 100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking for file: /content/drive/MyDrive/Npy_Files/Impressionism_training_data_160_160.npy\n",
            "Loading previous training pickle...\n",
            "Starting training...\n",
            "Epoch 1, gen loss=-1192.6748352050781, disc loss=-722.5265344619752, disc loss=-834.8283790588379, epoch time=0:02:25.35\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/honours_project/GAN_Images/Impressionism_Models/generator_model_160_res_5_seed_800_epochs_6000_batchSize_32_bufferSize_60000.npy/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/honours_project/GAN_Images/Impressionism_Models/discriminator_model_160_res_5_seed_800_epochs_6000_batchSize_32_bufferSize_60000.npy/assets\n",
            "Epoch 2, gen loss=-2660.04736328125, disc loss=-2154.6103515625, disc loss=-2281.498583984375, epoch time=0:02:25.66\n",
            "Epoch 3, gen loss=-4075.85986328125, disc loss=-3572.66689453125, disc loss=-3724.1443603515627, epoch time=0:02:23.05\n",
            "Epoch 4, gen loss=-5547.2060546875, disc loss=-5027.226171875, disc loss=-5163.644482421875, epoch time=0:02:23.79\n",
            "Epoch 5, gen loss=-7069.307373046875, disc loss=-6518.843359375, disc loss=-6678.2359375, epoch time=0:02:23.12\n",
            "Epoch 6, gen loss=-8647.09130859375, disc loss=-8111.823828125, disc loss=-8263.238232421874, epoch time=0:02:23.53\n",
            "Epoch 7, gen loss=-10291.10595703125, disc loss=-9731.06318359375, disc loss=-9876.5423828125, epoch time=0:02:23.41\n",
            "Epoch 8, gen loss=-11967.15625, disc loss=-11390.28095703125, disc loss=-11540.426953125, epoch time=0:02:22.83\n",
            "Epoch 9, gen loss=-13692.78466796875, disc loss=-13106.622265625, disc loss=-13259.6484375, epoch time=0:02:22.80\n",
            "Epoch 10, gen loss=-15489.8798828125, disc loss=-14878.973046874999, disc loss=-15039.56318359375, epoch time=0:02:23.31\n",
            "Epoch 11, gen loss=-17310.158203125, disc loss=-16697.729296875, disc loss=-16852.32158203125, epoch time=0:02:23.04\n",
            "Epoch 12, gen loss=-19186.2001953125, disc loss=-18564.875390625, disc loss=-18721.7142578125, epoch time=0:02:23.00\n",
            "Epoch 13, gen loss=-21076.7509765625, disc loss=-20472.9587890625, disc loss=-20641.7015625, epoch time=0:02:22.99\n",
            "Epoch 14, gen loss=-23078.2177734375, disc loss=-22437.9888671875, disc loss=-22600.07109375, epoch time=0:02:22.55\n",
            "Epoch 15, gen loss=-25105.2880859375, disc loss=-24418.025, disc loss=-24547.7958984375, epoch time=0:02:23.41\n",
            "Epoch 16, gen loss=-27155.369140625, disc loss=-26516.655468750003, disc loss=-26678.8388671875, epoch time=0:02:23.07\n",
            "Epoch 17, gen loss=-29298.0478515625, disc loss=-28609.269140625, disc loss=-28753.0853515625, epoch time=0:02:22.74\n",
            "Epoch 18, gen loss=-31459.3837890625, disc loss=-30770.837109375003, disc loss=-30938.7279296875, epoch time=0:02:22.40\n",
            "Epoch 19, gen loss=-33702.015625, disc loss=-32972.255078125, disc loss=-33147.5111328125, epoch time=0:02:22.96\n",
            "Epoch 20, gen loss=-35941.564453125, disc loss=-35205.938671875, disc loss=-35398.955468750006, epoch time=0:02:22.82\n",
            "Epoch 21, gen loss=-38271.578125, disc loss=-37526.709375, disc loss=-37700.642578125, epoch time=0:02:22.58\n",
            "Epoch 22, gen loss=-40608.03125, disc loss=-39868.390625, disc loss=-40053.786328125, epoch time=0:02:23.20\n",
            "Epoch 23, gen loss=-43072.23828125, disc loss=-42247.387890625, disc loss=-42456.48359375, epoch time=0:02:23.06\n",
            "Epoch 24, gen loss=-45475.275390625, disc loss=-44709.591015625, disc loss=-44900.45390625, epoch time=0:02:22.93\n",
            "Epoch 25, gen loss=-47964.07421875, disc loss=-47186.761328125, disc loss=-47384.65, epoch time=0:02:23.07\n",
            "Epoch 26, gen loss=-50560.3515625, disc loss=-49728.584375, disc loss=-49937.00078125, epoch time=0:02:23.28\n",
            "Epoch 27, gen loss=-53124.224609375, disc loss=-52319.47734375, disc loss=-52505.851171874994, epoch time=0:02:23.83\n",
            "Epoch 28, gen loss=-55755.2421875, disc loss=-54949.019140625, disc loss=-55144.855078124994, epoch time=0:02:23.11\n",
            "Epoch 29, gen loss=-58498.76953125, disc loss=-57616.134765625, disc loss=-57832.991015625, epoch time=0:02:23.53\n",
            "Epoch 30, gen loss=-61231.654296875, disc loss=-60356.814843750006, disc loss=-60574.031640625006, epoch time=0:02:23.16\n",
            "Epoch 31, gen loss=-64013.58203125, disc loss=-63118.728515625, disc loss=-63342.122265625, epoch time=0:02:23.16\n",
            "Epoch 32, gen loss=-66815.6875, disc loss=-65925.199609375, disc loss=-66160.46796875, epoch time=0:02:23.17\n",
            "Epoch 33, gen loss=-69725.0390625, disc loss=-68822.43359375, disc loss=-69049.58203125, epoch time=0:02:23.15\n",
            "Epoch 34, gen loss=-72671.21484375, disc loss=-71728.640625, disc loss=-71960.53359375, epoch time=0:02:23.01\n",
            "Epoch 35, gen loss=-75642.4140625, disc loss=-74694.4390625, disc loss=-74938.47578125, epoch time=0:02:23.33\n",
            "Epoch 36, gen loss=-78644.7265625, disc loss=-77713.81640625, disc loss=-77948.01015625, epoch time=0:02:23.29\n",
            "Epoch 37, gen loss=-81749.6640625, disc loss=-80778.453125, disc loss=-81017.28125, epoch time=0:02:22.99\n",
            "Epoch 38, gen loss=-84881.8125, disc loss=-83856.10546875, disc loss=-84110.68281249999, epoch time=0:02:23.06\n",
            "Epoch 39, gen loss=-88015.640625, disc loss=-87020.07265625, disc loss=-87272.81875, epoch time=0:02:23.29\n",
            "Epoch 40, gen loss=-91137.828125, disc loss=-90186.8546875, disc loss=-90466.4609375, epoch time=0:02:23.15\n",
            "Epoch 41, gen loss=-94544.4140625, disc loss=-93486.16093750001, disc loss=-93721.61953125, epoch time=0:02:23.38\n",
            "Epoch 42, gen loss=-97889.1875, disc loss=-96784.34453125, disc loss=-97023.81015625, epoch time=0:02:22.83\n",
            "Epoch 43, gen loss=-101214.9375, disc loss=-100133.66015625, disc loss=-100395.98125000001, epoch time=0:02:22.33\n",
            "Epoch 44, gen loss=-104594.29296875, disc loss=-103476.74453125, disc loss=-103794.77578125, epoch time=0:02:23.18\n",
            "Epoch 45, gen loss=-108016.08203125, disc loss=-106970.80859375, disc loss=-107223.18203125, epoch time=0:02:23.24\n",
            "Epoch 46, gen loss=-111580.609375, disc loss=-110421.51640625, disc loss=-110734.63046875, epoch time=0:02:23.14\n",
            "Epoch 47, gen loss=-115125.7890625, disc loss=-113946.08046875, disc loss=-114258.29140625, epoch time=0:02:23.31\n",
            "Epoch 48, gen loss=-118624.1171875, disc loss=-117550.9234375, disc loss=-117830.1390625, epoch time=0:02:23.63\n",
            "Epoch 49, gen loss=-122325.1484375, disc loss=-121112.2140625, disc loss=-121438.45546875, epoch time=0:02:23.32\n",
            "Epoch 50, gen loss=-126047.34375, disc loss=-124857.11640624999, disc loss=-125125.1, epoch time=0:02:23.23\n",
            "Epoch 51, gen loss=-129714.69921875, disc loss=-128522.86640625, disc loss=-128877.66953125, epoch time=0:02:24.10\n",
            "Epoch 52, gen loss=-133578.8125, disc loss=-132338.86875000002, disc loss=-132644.7796875, epoch time=0:02:23.69\n",
            "Epoch 53, gen loss=-137343.453125, disc loss=-136123.22031250002, disc loss=-136434.65, epoch time=0:02:22.99\n",
            "Epoch 54, gen loss=-141197.75, disc loss=-139991.5609375, disc loss=-140282.3875, epoch time=0:02:23.07\n",
            "Epoch 55, gen loss=-145154.46875, disc loss=-143888.9984375, disc loss=-144251.903125, epoch time=0:02:23.36\n",
            "Epoch 56, gen loss=-149168.8515625, disc loss=-147820.465625, disc loss=-148193.184375, epoch time=0:02:23.36\n",
            "Epoch 57, gen loss=-153159.203125, disc loss=-151867.871875, disc loss=-152173.5828125, epoch time=0:02:24.05\n",
            "Epoch 58, gen loss=-157164.34375, disc loss=-155913.546875, disc loss=-156231.203125, epoch time=0:02:23.30\n",
            "Epoch 59, gen loss=-161307.2578125, disc loss=-160031.8984375, disc loss=-160348.5265625, epoch time=0:02:22.96\n",
            "Epoch 60, gen loss=-165501.8046875, disc loss=-164136.3171875, disc loss=-164519.66249999998, epoch time=0:02:23.01\n",
            "Epoch 61, gen loss=-169702.5234375, disc loss=-168328.684375, disc loss=-168708.92812499998, epoch time=0:02:23.34\n",
            "Epoch 62, gen loss=-174013.1875, disc loss=-172581.78281250002, disc loss=-172959.865625, epoch time=0:02:23.19\n",
            "Epoch 63, gen loss=-178259.6796875, disc loss=-176818.375, disc loss=-177239.871875, epoch time=0:02:23.15\n",
            "Epoch 64, gen loss=-182645.6875, disc loss=-181186.96875, disc loss=-181573.32968750002, epoch time=0:02:23.35\n",
            "Epoch 65, gen loss=-186984.578125, disc loss=-185573.603125, disc loss=-185944.453125, epoch time=0:02:22.86\n",
            "Epoch 66, gen loss=-191472.984375, disc loss=-189962.925, disc loss=-190351.73125, epoch time=0:02:22.89\n",
            "Epoch 67, gen loss=-195946.8046875, disc loss=-194467.11875, disc loss=-194858.2859375, epoch time=0:02:22.78\n",
            "Epoch 68, gen loss=-200488.9375, disc loss=-199023.2390625, disc loss=-199389.5546875, epoch time=0:02:23.27\n",
            "Epoch 69, gen loss=-205083.859375, disc loss=-203561.3046875, disc loss=-203985.66562500002, epoch time=0:02:23.33\n",
            "Epoch 70, gen loss=-209773.6171875, disc loss=-208249.9421875, disc loss=-208592.515625, epoch time=0:02:23.00\n",
            "Epoch 71, gen loss=-214362.984375, disc loss=-212838.6921875, disc loss=-213276.6640625, epoch time=0:02:23.44\n",
            "Epoch 72, gen loss=-219172.2890625, disc loss=-217549.340625, disc loss=-217975.4265625, epoch time=0:02:23.27\n",
            "Epoch 73, gen loss=-223845.1953125, disc loss=-222280.40625, disc loss=-222746.4, epoch time=0:02:23.29\n",
            "Epoch 74, gen loss=-228697.5625, disc loss=-227118.1765625, disc loss=-227526.7859375, epoch time=0:02:22.86\n",
            "Epoch 75, gen loss=-233452.8125, disc loss=-231937.40468749998, disc loss=-232395.5953125, epoch time=0:02:23.50\n",
            "Epoch 76, gen loss=-238501.78125, disc loss=-236827.5546875, disc loss=-237294.759375, epoch time=0:02:23.32\n",
            "Epoch 77, gen loss=-243377.1953125, disc loss=-241802.565625, disc loss=-242244.68125, epoch time=0:02:24.17\n",
            "Epoch 78, gen loss=-248309.5625, disc loss=-246791.7109375, disc loss=-247257.79531249998, epoch time=0:02:23.44\n",
            "Epoch 79, gen loss=-253527.6171875, disc loss=-251814.5625, disc loss=-252270.1734375, epoch time=0:02:23.67\n",
            "Epoch 80, gen loss=-258475.09375, disc loss=-256867.2078125, disc loss=-257387.60312500002, epoch time=0:02:23.87\n",
            "Epoch 81, gen loss=-263711.515625, disc loss=-262011.90468749998, disc loss=-262508.0359375, epoch time=0:02:23.80\n",
            "Epoch 82, gen loss=-268910.75, disc loss=-267129.26875, disc loss=-267665.0625, epoch time=0:02:23.80\n",
            "Epoch 83, gen loss=-274264.203125, disc loss=-272474.31875, disc loss=-272941.228125, epoch time=0:02:23.94\n",
            "Epoch 84, gen loss=-279410.59375, disc loss=-277769.9, disc loss=-278223.990625, epoch time=0:02:23.74\n",
            "Epoch 85, gen loss=-284721.875, disc loss=-283024.159375, disc loss=-283482.475, epoch time=0:02:23.72\n",
            "Epoch 86, gen loss=-290163.28125, disc loss=-288380.425, disc loss=-288918.4625, epoch time=0:02:23.28\n",
            "Epoch 87, gen loss=-295587.78125, disc loss=-293740.91875, disc loss=-294301.671875, epoch time=0:02:23.06\n",
            "Epoch 88, gen loss=-301090.953125, disc loss=-299276.271875, disc loss=-299788.934375, epoch time=0:02:23.50\n",
            "Epoch 89, gen loss=-306650.625, disc loss=-304735.321875, disc loss=-305328.09062499995, epoch time=0:02:23.56\n",
            "Epoch 90, gen loss=-312157.453125, disc loss=-310330.2875, disc loss=-310826.109375, epoch time=0:02:23.41\n",
            "Epoch 91, gen loss=-317768.984375, disc loss=-315973.41875, disc loss=-316503.396875, epoch time=0:02:23.62\n",
            "Epoch 92, gen loss=-323597.578125, disc loss=-321645.5375, disc loss=-322186.35, epoch time=0:02:23.55\n",
            "Epoch 93, gen loss=-329233.90625, disc loss=-327268.778125, disc loss=-327793.0625, epoch time=0:02:24.36\n",
            "Epoch 94, gen loss=-334883.328125, disc loss=-333018.490625, disc loss=-333549.809375, epoch time=0:02:23.44\n",
            "Epoch 95, gen loss=-340766.65625, disc loss=-338821.140625, disc loss=-339299.390625, epoch time=0:02:23.77\n",
            "Epoch 96, gen loss=-346513.8125, disc loss=-344642.4875, disc loss=-345166.365625, epoch time=0:02:23.21\n",
            "Epoch 97, gen loss=-352486.875, disc loss=-350464.01249999995, disc loss=-351114.853125, epoch time=0:02:23.35\n",
            "Epoch 98, gen loss=-358430.609375, disc loss=-356487.1625, disc loss=-357032.425, epoch time=0:02:23.88\n",
            "Epoch 99, gen loss=-364544.4375, disc loss=-362281.575, disc loss=-362998.15625, epoch time=0:02:23.23\n",
            "Epoch 100, gen loss=-370420.46875, disc loss=-368300.503125, disc loss=-369028.725, epoch time=0:02:23.54\n",
            "Epoch 101, gen loss=-376567.796875, disc loss=-374439.30625, disc loss=-375080.2375, epoch time=0:02:23.15\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/honours_project/GAN_Images/Impressionism_Models/generator_model_160_res_5_seed_800_epochs_6000_batchSize_32_bufferSize_60000.npy/assets\n",
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/Colab Notebooks/honours_project/GAN_Images/Impressionism_Models/discriminator_model_160_res_5_seed_800_epochs_6000_batchSize_32_bufferSize_60000.npy/assets\n",
            "Epoch 102, gen loss=-382610.5625, disc loss=-380622.634375, disc loss=-381276.853125, epoch time=0:02:23.46\n",
            "Epoch 103, gen loss=-388903.703125, disc loss=-386828.609375, disc loss=-387335.646875, epoch time=0:02:23.31\n",
            "Epoch 104, gen loss=-395180.03125, disc loss=-393032.6875, disc loss=-393640.4125, epoch time=0:02:23.56\n",
            "Epoch 105, gen loss=-401223.515625, disc loss=-399256.54375, disc loss=-399800.34375, epoch time=0:02:23.38\n",
            "Epoch 106, gen loss=-407757.515625, disc loss=-405500.546875, disc loss=-406150.915625, epoch time=0:02:23.28\n",
            "Epoch 107, gen loss=-413977.09375, disc loss=-411934.725, disc loss=-412517.859375, epoch time=0:02:23.23\n",
            "Epoch 108, gen loss=-420499.03125, disc loss=-418379.84375, disc loss=-418921.846875, epoch time=0:02:23.12\n",
            "Epoch 109, gen loss=-426980.984375, disc loss=-424613.171875, disc loss=-425337.66874999995, epoch time=0:02:23.48\n",
            "Epoch 110, gen loss=-433510.140625, disc loss=-431126.90625, disc loss=-431854.18125, epoch time=0:02:23.49\n",
            "Epoch 111, gen loss=-439809.984375, disc loss=-437773.24375, disc loss=-438402.8375, epoch time=0:02:23.02\n",
            "Epoch 112, gen loss=-446637.515625, disc loss=-444405.37187499995, disc loss=-445050.584375, epoch time=0:02:23.23\n",
            "Epoch 113, gen loss=-453206.875, disc loss=-450883.45625, disc loss=-451684.46875, epoch time=0:02:23.67\n",
            "Epoch 114, gen loss=-459867.53125, disc loss=-457663.18437499995, disc loss=-458276.37812500005, epoch time=0:02:23.53\n",
            "Epoch 115, gen loss=-466814.0625, disc loss=-464284.428125, disc loss=-465034.86875, epoch time=0:02:23.48\n",
            "Epoch 116, gen loss=-473468.234375, disc loss=-471155.59687500005, disc loss=-471801.48125, epoch time=0:02:23.32\n",
            "Epoch 117, gen loss=-480232.296875, disc loss=-477915.828125, disc loss=-478635.0125, epoch time=0:02:23.52\n",
            "Epoch 118, gen loss=-487223.0625, disc loss=-484923.17500000005, disc loss=-485463.12187499995, epoch time=0:02:23.39\n",
            "Epoch 119, gen loss=-494031.5, disc loss=-491773.734375, disc loss=-492401.61875, epoch time=0:02:23.42\n",
            "Epoch 120, gen loss=-501178.890625, disc loss=-498419.725, disc loss=-499337.58125, epoch time=0:02:23.39\n",
            "Epoch 121, gen loss=-508096.5625, disc loss=-505522.134375, disc loss=-506324.240625, epoch time=0:02:23.23\n",
            "Epoch 122, gen loss=-515391.671875, disc loss=-512794.240625, disc loss=-513484.034375, epoch time=0:02:23.18\n",
            "Epoch 123, gen loss=-522180.734375, disc loss=-519688.309375, disc loss=-520540.971875, epoch time=0:02:23.69\n",
            "Epoch 124, gen loss=-529585.5625, disc loss=-526871.075, disc loss=-527659.075, epoch time=0:02:23.37\n",
            "Epoch 125, gen loss=-536736.78125, disc loss=-533955.525, disc loss=-534927.175, epoch time=0:02:23.40\n",
            "Epoch 126, gen loss=-544121.3125, disc loss=-541386.21875, disc loss=-542132.41875, epoch time=0:02:23.24\n",
            "Epoch 127, gen loss=-550999.1875, disc loss=-548615.8125, disc loss=-549362.76875, epoch time=0:02:22.49\n",
            "Epoch 128, gen loss=-558426.71875, disc loss=-555866.8625, disc loss=-556761.1000000001, epoch time=0:02:22.75\n",
            "Epoch 129, gen loss=-565599.375, disc loss=-563257.5875, disc loss=-564068.94375, epoch time=0:02:22.75\n",
            "Epoch 130, gen loss=-573255.5, disc loss=-570740.5875, disc loss=-571403.59375, epoch time=0:02:23.28\n",
            "Epoch 131, gen loss=-580815.1875, disc loss=-578162.125, disc loss=-578967.75, epoch time=0:02:23.78\n",
            "Epoch 132, gen loss=-588323.25, disc loss=-585741.14375, disc loss=-586385.4375, epoch time=0:02:23.59\n",
            "Epoch 133, gen loss=-595945.1875, disc loss=-593211.4312499999, disc loss=-594077.6, epoch time=0:02:23.65\n",
            "Epoch 134, gen loss=-603471.375, disc loss=-600643.275, disc loss=-601581.94375, epoch time=0:02:23.12\n",
            "Epoch 135, gen loss=-610981.1875, disc loss=-608448.3125, disc loss=-609278.275, epoch time=0:02:23.48\n",
            "Epoch 136, gen loss=-619044.0, disc loss=-616248.25, disc loss=-616924.86875, epoch time=0:02:23.18\n",
            "Epoch 137, gen loss=-626521.9375, disc loss=-624115.09375, disc loss=-624773.1875, epoch time=0:02:23.35\n",
            "Epoch 138, gen loss=-634237.71875, disc loss=-631696.2125, disc loss=-632499.625, epoch time=0:02:23.53\n",
            "Epoch 139, gen loss=-642222.71875, disc loss=-639311.2625, disc loss=-640402.7437499999, epoch time=0:02:23.20\n",
            "Epoch 140, gen loss=-650203.5, disc loss=-647363.4624999999, disc loss=-648221.33125, epoch time=0:02:23.32\n",
            "Epoch 141, gen loss=-658055.65625, disc loss=-655318.28125, disc loss=-656275.1375, epoch time=0:02:23.02\n",
            "Epoch 142, gen loss=-666243.3125, disc loss=-663145.3500000001, disc loss=-664247.4625, epoch time=0:02:23.22\n",
            "Epoch 143, gen loss=-674062.5, disc loss=-671022.91875, disc loss=-672224.925, epoch time=0:02:23.68\n",
            "Epoch 144, gen loss=-682184.875, disc loss=-679356.54375, disc loss=-680064.51875, epoch time=0:02:23.57\n",
            "Epoch 145, gen loss=-690484.4375, disc loss=-687412.63125, disc loss=-688216.1625000001, epoch time=0:02:23.31\n",
            "Epoch 146, gen loss=-698452.0625, disc loss=-695527.7437499999, disc loss=-696514.28125, epoch time=0:02:23.63\n",
            "Epoch 147, gen loss=-706615.6875, disc loss=-703799.8187500001, disc loss=-704697.0, epoch time=0:02:23.43\n",
            "Epoch 148, gen loss=-715211.625, disc loss=-711976.05, disc loss=-713058.5249999999, epoch time=0:02:23.54\n",
            "Epoch 149, gen loss=-723167.65625, disc loss=-720409.99375, disc loss=-721363.81875, epoch time=0:02:23.67\n",
            "Epoch 150, gen loss=-731922.8125, disc loss=-728639.875, disc loss=-729679.0625, epoch time=0:02:23.26\n",
            "Epoch 151, gen loss=-739797.75, disc loss=-737135.825, disc loss=-737969.0, epoch time=0:02:23.45\n",
            "Epoch 152, gen loss=-748480.5625, disc loss=-745342.66875, disc loss=-746450.78125, epoch time=0:02:23.56\n",
            "Epoch 153, gen loss=-757049.75, disc loss=-754035.03125, disc loss=-754936.0874999999, epoch time=0:02:23.52\n",
            "Epoch 154, gen loss=-765324.84375, disc loss=-762444.40625, disc loss=-763319.91875, epoch time=0:02:23.41\n",
            "Epoch 155, gen loss=-774299.0625, disc loss=-771091.3, disc loss=-771978.175, epoch time=0:02:23.89\n",
            "Epoch 156, gen loss=-782559.125, disc loss=-779933.64375, disc loss=-780681.53125, epoch time=0:02:23.48\n",
            "Epoch 157, gen loss=-791399.65625, disc loss=-788261.4624999999, disc loss=-789279.0875, epoch time=0:02:23.70\n",
            "Epoch 158, gen loss=-799951.53125, disc loss=-797057.81875, disc loss=-798012.04375, epoch time=0:02:23.71\n",
            "Epoch 159, gen loss=-808748.65625, disc loss=-805700.11875, disc loss=-806737.2125, epoch time=0:02:23.39\n",
            "Epoch 160, gen loss=-817436.21875, disc loss=-814215.79375, disc loss=-815597.09375, epoch time=0:02:23.72\n",
            "Epoch 161, gen loss=-826569.375, disc loss=-823543.71875, disc loss=-824392.66875, epoch time=0:02:23.31\n",
            "Epoch 162, gen loss=-835144.125, disc loss=-832171.6375, disc loss=-833161.98125, epoch time=0:02:23.76\n",
            "Epoch 163, gen loss=-844944.15625, disc loss=-841180.44375, disc loss=-842114.65, epoch time=0:02:23.45\n",
            "Epoch 164, gen loss=-853217.875, disc loss=-849914.54375, disc loss=-851153.3125, epoch time=0:02:23.16\n",
            "Epoch 165, gen loss=-862494.46875, disc loss=-858971.7124999999, disc loss=-860322.3500000001, epoch time=0:02:23.42\n",
            "Epoch 166, gen loss=-871415.0625, disc loss=-868238.6125, disc loss=-869221.0687500001, epoch time=0:02:23.85\n",
            "Epoch 167, gen loss=-880552.34375, disc loss=-877321.725, disc loss=-878587.83125, epoch time=0:02:23.50\n",
            "Epoch 168, gen loss=-890038.3125, disc loss=-886342.2437499999, disc loss=-887515.75, epoch time=0:02:23.41\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}