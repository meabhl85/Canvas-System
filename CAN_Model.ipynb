{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CAN-Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN6rZNBbEOFkhL1w3QjqjV1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meabhl85/Canvas-System/blob/main/CAN_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7k6lFXEepEC"
      },
      "source": [
        "#### Imports and Google Drive Connection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFZafEk_YxSj",
        "outputId": "09d3582f-b412-40f8-946b-43c5f6df5d11"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Reshape, Dropout, Dense \n",
        "from tensorflow.keras.layers import Flatten, BatchNormalization\n",
        "from tensorflow.keras.layers import Activation, ZeroPadding2D\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.constraints import Constraint\n",
        "from tensorflow.keras import backend\n",
        "import numpy as np\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from numpy import expand_dims\n",
        "from numpy import mean\n",
        "from numpy import ones\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import os \n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    COLAB = True\n",
        "    print(\"Note: using Google CoLab\")\n",
        "    %tensorflow_version 2.x\n",
        "except:\n",
        "    print(\"Note: not using Google CoLab\")\n",
        "    COLAB = False\n",
        "\n",
        "#Nicely formatted time string\n",
        "def hms_string(sec_elapsed):\n",
        "  h = int(sec_elapsed / (60 * 60))\n",
        "  m = int((sec_elapsed % (60 * 60)) / 60)\n",
        "  s = sec_elapsed % 60\n",
        "  return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Note: using Google CoLab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TntaaV_HeuQX"
      },
      "source": [
        "#### Model Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kknpzagDYznZ"
      },
      "source": [
        "#Image Sizes\n",
        "GENERATE_RES = 5 # Generation resolution factor \n",
        "GENERATE_SQUARE = 32 * GENERATE_RES # rows/cols (should be square)\n",
        "IMAGE_CHANNELS = 3\n",
        "\n",
        "#Input and Output Paths\n",
        "NPY_FILES_PATH = '/content/drive/MyDrive/Colab Notebooks/honours_project/Npy_Files/'\n",
        "OUTPUT_IMAGE_PATH = '/content/drive/MyDrive/Colab Notebooks/honours_project/GAN_Images/'\n",
        "\n",
        "# Preview image \n",
        "PREVIEW_ROWS = 2\n",
        "PREVIEW_COLS = 3\n",
        "PREVIEW_MARGIN = 16\n",
        "\n",
        "# Size vector to generate images from\n",
        "SEED_SIZE = 800\n",
        "EPOCHS = 1000\n",
        "BATCH_SIZE = 32\n",
        "BUFFER_SIZE = 60000"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlIJPkFIe5lD"
      },
      "source": [
        "#### Model Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w2TBkgaYY1-F"
      },
      "source": [
        "class CAN:\n",
        "  def __init__(self):\n",
        "    self.class_count = 0\n",
        "    class_names = []\n",
        "    init = True\n",
        "    self.image_shape = (GENERATE_SQUARE, GENERATE_SQUARE, IMAGE_CHANNELS)\n",
        "    #Define cross_entropy function\n",
        "    self.cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
        "\n",
        "    for filename in tqdm(os.listdir(NPY_FILES_PATH)):\n",
        "      npy_file = os.path.join(NPY_FILES_PATH, filename)\n",
        "      class_names.append(filename)\n",
        "      AC_training_data = np.load(npy_file)\n",
        "      AC_labels = [self.class_count] * len(AC_training_data)\n",
        "      AC_labels = np.array(AC_labels)\n",
        "\n",
        "      if init == True:\n",
        "        self.training_data = AC_training_data\n",
        "        self.training_labels = AC_labels\n",
        "        init == False\n",
        "      else:  \n",
        "        #Combine files\n",
        "        self.training_data = np.concatenate([self.training_data, AC_training_data])\n",
        "        self.training_labels = np.concatenate([self.training_labels, AC_labels])\n",
        "      break\n",
        "    self.training_dataset = tf.data.Dataset.from_tensor_slices(self.training_data).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)  \n",
        "    self.training_labels = tf.data.Dataset.from_tensor_slices(self.training_labels).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "    self.class_count += 1\n",
        "\n",
        "\n",
        "  def build_generator(self):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(4*4*256, activation = \"relu\", input_dim = SEED_SIZE))\n",
        "    model.add(Reshape((4, 4, 256)))\n",
        "\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(256, kernel_size = 3, padding = \"same\"))\n",
        "    model.add(BatchNormalization(momentum = 0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(256, kernel_size = 3, padding = \"same\"))\n",
        "    model.add(BatchNormalization(momentum = 0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "   \n",
        "    # Output resolution, additional upsampling\n",
        "    model.add(UpSampling2D())\n",
        "    model.add(Conv2D(128, kernel_size = 3, padding = \"same\"))\n",
        "    model.add(BatchNormalization(momentum = 0.8))\n",
        "    model.add(Activation(\"relu\"))\n",
        "\n",
        "    if GENERATE_RES > 1:\n",
        "      model.add(UpSampling2D(size = (GENERATE_RES, GENERATE_RES)))\n",
        "      model.add(Conv2D(128,kernel_size=3, padding=\"same\"))\n",
        "      model.add(BatchNormalization(momentum = 0.8))\n",
        "      model.add(Activation(\"relu\"))\n",
        "\n",
        "    # Final CNN layer\n",
        "    model.add(Conv2D(IMAGE_CHANNELS, kernel_size=3, padding=\"same\"))\n",
        "    model.add(Activation(\"tanh\"))\n",
        "\n",
        "    return model\n",
        "\n",
        "  def build_discriminator(self):\n",
        "    \n",
        "    init = RandomNormal(stddev=0.02)\n",
        "    inputs = tf.keras.Input(shape=self.image_shape)\n",
        "\n",
        "    x = tf.random.normal(self.image_shape)\n",
        "\n",
        "    #Layer 1\n",
        "    # layer_1 = tf.keras.layers.Conv2D(32, kernel_size = 3, strides = 2, padding = \"same\", input_shape=image_shape[1:])(x)\n",
        "    layer_1 = tf.keras.layers.Conv2D(32, kernel_size = 3, kernel_initializer=init, strides = 2, padding = \"same\")(inputs)\n",
        "    relu_1 = tf.keras.layers.LeakyReLU(alpha = 0.2)(layer_1)\n",
        "\n",
        "    #Layer 2\n",
        "    dropout_1 = tf.keras.layers.Dropout(0.25)(relu_1)\n",
        "    layer_2 = tf.keras.layers.Conv2D(64, kernel_size = 3, kernel_initializer=init, strides = 2, padding = \"same\")(dropout_1)\n",
        "    zero_padding_1 = tf.keras.layers.ZeroPadding2D(padding = ((0, 1), (0, 1)))(layer_2)\n",
        "    batch_norm_1 = tf.keras.layers.BatchNormalization(momentum = 0.8)(zero_padding_1)\n",
        "    relu_2 = tf.keras.layers.LeakyReLU(alpha = 0.2)(batch_norm_1)\n",
        "\n",
        "    #Layer 3\n",
        "    dropout_2 = tf.keras.layers.Dropout(0.25)(relu_2)\n",
        "    layer_3 = tf.keras.layers.Conv2D(128, kernel_size = 3, kernel_initializer=init, strides = 2, padding = \"same\")(dropout_2)\n",
        "    batch_norm_2 = tf.keras.layers.BatchNormalization(momentum = 0.8)(layer_3)\n",
        "    relu_3 = tf.keras.layers.LeakyReLU(alpha = 0.2)(batch_norm_2)\n",
        "\n",
        "    #Layer 4 \n",
        "    dropout_3 = tf.keras.layers.Dropout(0.25)(relu_3)\n",
        "    layer_4 = tf.keras.layers.Conv2D(256, kernel_size = 3, kernel_initializer=init, strides = 1, padding = \"same\")(dropout_3)\n",
        "    batch_norm_3 = tf.keras.layers.BatchNormalization(momentum = 0.8)(layer_4)\n",
        "    relu_4 = tf.keras.layers.LeakyReLU(alpha = 0.2)(batch_norm_3)\n",
        "\n",
        "    #Layer 5\n",
        "    dropout_4 = tf.keras.layers.Dropout(0.25)(relu_4)\n",
        "    layer_5 = tf.keras.layers.Conv2D(512, kernel_size = 3, kernel_initializer=init, strides = 1, padding = \"same\")(dropout_4)\n",
        "    batch_norm_4 = tf.keras.layers.BatchNormalization(momentum = 0.8)(layer_5)\n",
        "    relu_5 = tf.keras.layers.LeakyReLU(alpha = 0.2)(batch_norm_4)\n",
        "\n",
        "    #Real/Fake Output\n",
        "    dropout_5 = tf.keras.layers.Dropout(0.25)(relu_5)\n",
        "    flatten_1 = tf.keras.layers.Flatten()(dropout_5)\n",
        "    discriminator_output = tf.keras.layers.Dense(1, kernel_initializer=init, name=\"RF_Output\")(flatten_1)\n",
        "\n",
        "    #Layer 7\n",
        "    layer_7 = tf.keras.layers.Dense(1024, kernel_initializer=init, name = 'd_h8_lin')(relu_5)\n",
        "    relu_7 = tf.keras.layers.LeakyReLU(alpha = 0.2)(layer_7)\n",
        "\n",
        "    #Layer 8\n",
        "    layer_8 = tf.keras.layers.Dense(512, kernel_initializer=init, name = 'd_h9_lin')(relu_7)\n",
        "    relu_7 = tf.keras.layers.LeakyReLU(alpha = 0.2)(layer_8)\n",
        "    \n",
        "    flatten_2 = tf.keras.layers.Flatten()(relu_7)\n",
        "    discriminator_class_output = tf.keras.layers.Dense(self.class_count, kernel_initializer=init, name = 'd_co_lin')(flatten_2)\n",
        "    discriminator_class_output_softmax = tf.nn.softmax(discriminator_class_output)   \n",
        "\n",
        "    # #Class Output\n",
        "    # output_2 = tf.keras.layers.Dense(class_count, activation=tf.nn.softmax, name=\"Class_Output\")(discriminator_class_output)\n",
        "    model = tf.keras.Model(inputs = inputs, outputs = [tf.nn.sigmoid(discriminator_output), discriminator_output, discriminator_class_output, discriminator_class_output_softmax])\n",
        "\n",
        "    return model\n",
        "\n",
        "  def discriminator_rf_loss(self, disc_output_linear, disc_output_sigmoid):\n",
        "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = disc_output_linear, \n",
        "                                                                labels = tf.ones_like(disc_output_sigmoid)))\n",
        "\n",
        "  def discriminator_class_loss(self, disc_class_output, training_class_output):\n",
        "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits = disc_class_output, \n",
        "                                                                  labels = training_class_output))\n",
        "\n",
        "  # def generator_class_loss(disc_class_output, disc_class_output_softmax):\n",
        "  #   return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = disc_class_output,\n",
        "                                                                  # labels = (1.0 / 27) * tf.ones_like(disc_class_output_softmax)))\n",
        "\n",
        "  def generator_class_loss(self, disc_class_output, disc_class_output_softmax):\n",
        "    # training_labels = tf.convert_to_tensor(, np.float32)\n",
        "    return tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = disc_class_output,\n",
        "                                                                  labels = (1.0 / self.class_count) * tf.ones_like(disc_class_output_softmax)))\n",
        "\n",
        "  def discriminator_total_loss(self, real_disc_output_linear, real_disc_output_sigmoid, fake_disc_output_linear, fake_disc_output_sigmoid, real_image_labels, real_disc_class_output):\n",
        "      discriminator_real_loss = discriminator_rf_loss(real_disc_output_linear, real_disc_output_sigmoid)\n",
        "      discriminator_fake_loss = discriminator_rf_loss(fake_disc_output_linear, fake_disc_output_sigmoid)\n",
        "\n",
        "      real_image_labels = tf.cast(real_image_labels, tf.float32)\n",
        "      real_disc_class_output = tf.convert_to_tensor(real_disc_class_output, np.float32)\n",
        "\n",
        "      discriminator_loss_class_real = discriminator_class_loss(real_disc_class_output, real_image_labels)\n",
        "\n",
        "      discriminator_l = discriminator_real_loss + discriminator_fake_loss + discriminator_loss_class_real\n",
        "\n",
        "      return discriminator_l\n",
        "\n",
        "  def generator_loss(self, fake_pred):\n",
        "    #As G wants to create real images the loss is cross entropy of 1's and the generated output\n",
        "    return self.cross_entropy(tf.ones_like(fake_pred), fake_pred)\n",
        "\n",
        "  def discriminator_loss(self, real_pred, fake_pred):\n",
        "    #D should compare the real predictions to 1 and fake to 0 (as they are the real values)\n",
        "    real_image_loss = self.cross_entropy(tf.ones_like(real_pred), real_pred)\n",
        "    fake_image_loss = self.cross_entropy(tf.zeros_like(fake_pred), fake_pred)\n",
        "\n",
        "    #Discriminator uses the complete loss\n",
        "    final_loss = real_image_loss + fake_image_loss\n",
        "    return final_loss\n",
        "\n",
        "  def compile_models(self):\n",
        "    self.discriminator_optimiser = tf.keras.optimizers.Adam(0.0002, 0.5)\n",
        "    self.generator_optimiser = tf.keras.optimizers.Adam(0.0002, 0.5)\n",
        "\n",
        "  # @tf.function\n",
        "  def step(self, real_images, real_image_labels):\n",
        "    seed = tf.random.normal([BATCH_SIZE, SEED_SIZE])\n",
        "\n",
        "    #Performs automatic differentiation used for backpropagation \n",
        "    with tf.GradientTape() as g_tape, tf.GradientTape() as d_tape:\n",
        "      #Generate images with noise vector\n",
        "      generated_images = self.generator(seed, training = True)\n",
        "\n",
        "      #Put resulting image into the D and return prediction\n",
        "      real_disc_output_sigmoid, real_disc_output_linear, real_disc_class_output, real_disc_class_output_softmax = self.discriminator(real_images, training = True)\n",
        "      real_disc_class_output = np.argmax(real_disc_class_output, 1)\n",
        "      real_disc_class_output_softmax = np.argmax(real_disc_class_output_softmax, 1)\n",
        "\n",
        "      fake_disc_output_sigmoid, fake_disc_output_linear, fake_disc_class_output, fake_disc_class_output_softmax = self.discriminator(generated_images, training = True)\n",
        "      fake_disc_class_output = np.argmax(fake_disc_class_output, 1)\n",
        "      fake_disc_class_output_softmax = np.argmax(fake_disc_class_output_softmax, 1)\n",
        "\n",
        "      #Calculate loss for both D and G\n",
        "      # g_loss = generator_loss(fake_pred)\n",
        "      # d_loss = discriminator_loss(real_pred, fake_pred)\n",
        "\n",
        "      #DISCRIMINATOR LOSS\n",
        "      discriminator_real_loss = self.discriminator_rf_loss(real_disc_output_linear, real_disc_output_sigmoid)\n",
        "      discriminator_fake_loss = self.discriminator_rf_loss(fake_disc_output_linear, fake_disc_output_sigmoid)\n",
        "\n",
        "      real_image_labels = tf.cast(real_image_labels, tf.float32)\n",
        "      real_disc_class_output = tf.convert_to_tensor(real_disc_class_output, np.float32)\n",
        "      discriminator_loss_class_real = self.discriminator_class_loss(real_disc_class_output, real_image_labels)\n",
        "      \n",
        "      discriminator_loss = discriminator_real_loss + discriminator_fake_loss + discriminator_loss_class_real\n",
        "\n",
        "      #GENERATOR LOSS\n",
        "      a = tf.ones_like(fake_disc_class_output_softmax)\n",
        "      fake_disc_class_output_softmax_temp = (1.0 / 27) * a.numpy()\n",
        "      fake_disc_class_output_softmax_temp = tf.convert_to_tensor(fake_disc_class_output_softmax_temp, np.float32)\n",
        "      fake_disc_class_output = tf.convert_to_tensor(fake_disc_class_output, np.float32)\n",
        "\n",
        "      generator_loss_class_fake = self.generator_class_loss(fake_disc_class_output, fake_disc_class_output_softmax_temp)\n",
        "      generator_loss_fake = self.discriminator_rf_loss(fake_disc_output_linear, fake_disc_output_sigmoid)\n",
        "\n",
        "      generator_loss = generator_loss_fake + 1.0 * generator_loss_class_fake\n",
        "\n",
        "    #Calculate gradients\n",
        "    gen_gradients = g_tape.gradient(generator_loss, self.generator.trainable_variables)\n",
        "    dis_gradients = d_tape.gradient(discriminator_loss, self.discriminator.trainable_variables)\n",
        "    \n",
        "    # Update weights \n",
        "    self.generator_optimiser.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))\n",
        "    self.discriminator_optimiser.apply_gradients(zip(dis_gradients, self.discriminator.trainable_variables))\n",
        "\n",
        "    print(generator_loss)\n",
        "    print(discriminator_loss)\n",
        "\n",
        "    return generator_loss, discriminator_loss\n",
        "\n",
        "  def train(self, training_dataset, training_labels, epochs, save_img_checkpoint, save_model_checkpoint):\n",
        "    fixed_seed = np.random.normal(0, 1, (PREVIEW_ROWS * PREVIEW_COLS, SEED_SIZE))\n",
        "    start = time.time()\n",
        "    save_count = 0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      #Initalise timing and loss lists\n",
        "      epoch_time = time.time()\n",
        "      g_loss_list = []\n",
        "      d_loss_list = []\n",
        "\n",
        "      #Run the step training\n",
        "      for real_images, real_image_labels in zip(training_dataset, training_labels):\n",
        "        t = self.step(real_images, real_image_labels)\n",
        "        g_loss_list.append(t[0])\n",
        "        d_loss_list.append(t[1])\n",
        "      \n",
        "      \n",
        "      #Average losses for entire image batch\n",
        "      average_g_loss = sum(g_loss_list) / len(g_loss_list)\n",
        "      average_d_loss = sum(d_loss_list) / len(d_loss_list)\n",
        "\n",
        "      #Calculate epoch time\n",
        "      epoch_finished = time.time() - epoch_time\n",
        "      print (f'Epoch {epoch + 1}, gen loss={average_g_loss}, disc loss={average_d_loss}, epoch time={hms_string(epoch_finished)}')\n",
        "      \n",
        "      if save_count == 10:\n",
        "        self.save_images(epoch, fixed_seed)\n",
        "        save_count = 0\n",
        "\n",
        "      save_count += 1\n",
        "\n",
        "    final_time = time.time()-start\n",
        "    print (f'Training time: {hms_string(final_time)}')\n",
        "\n",
        "  def save_models(self):\n",
        "    output_model_path = os.path.join(OUTPUT_IMAGE_PATH, self.data_name + \"_Models\")\n",
        "    if not os.path.exists(output_model_path):\n",
        "      os.makedirs(output_model_path)\n",
        "\n",
        "    self.generator.save(os.path.join(output_model_path, f'generator_model_{GENERATE_SQUARE}_res_{GENERATE_RES}_seed_{SEED_SIZE}_epochs_{EPOCHS}_batchSize_{BATCH_SIZE}_bufferSize_{BUFFER_SIZE}.npy'))\n",
        "    self.discriminator.save(os.path.join(output_model_path, f'discriminator_model_{GENERATE_SQUARE}_res_{GENERATE_RES}_seed_{SEED_SIZE}_epochs_{EPOCHS}_batchSize_{BATCH_SIZE}_bufferSize_{BUFFER_SIZE}.npy'))\n",
        "\n",
        "  def load_models(self):\n",
        "    output_model_path = os.path.join(OUTPUT_IMAGE_PATH, self.data_name + \"_Models\")\n",
        "    self.generator = load_model(os.path.join(output_model_path, f'generator_model_{GENERATE_SQUARE}_res_{GENERATE_RES}_seed_{SEED_SIZE}_epochs_{EPOCHS}_batchSize_{BATCH_SIZE}_bufferSize_{BUFFER_SIZE}.npy'))\n",
        "    self.discriminator = load_model(os.path.join(output_model_path, f'discriminator_model_{GENERATE_SQUARE}_res_{GENERATE_RES}_seed_{SEED_SIZE}_epochs_{EPOCHS}_batchSize_{BATCH_SIZE}_bufferSize_{BUFFER_SIZE}.npy'))\n",
        "\n",
        "  def save_images(self, count, noise):\n",
        "    image_array = np.full(( \n",
        "        PREVIEW_MARGIN + (PREVIEW_ROWS * (GENERATE_SQUARE+PREVIEW_MARGIN)), \n",
        "        PREVIEW_MARGIN + (PREVIEW_COLS * (GENERATE_SQUARE+PREVIEW_MARGIN)), 3), \n",
        "        255, dtype=np.uint8)\n",
        "    \n",
        "    print(image_array.shape)\n",
        "    \n",
        "    generated_images = self.generator.predict(noise)\n",
        "\n",
        "    generated_images = 0.5 * generated_images + 0.5\n",
        "\n",
        "    image_count = 0\n",
        "    for row in range(PREVIEW_ROWS):\n",
        "        for col in range(PREVIEW_COLS):\n",
        "          \n",
        "          r = row * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n",
        "          c = col * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n",
        "\n",
        "          # print(\"Rows \", r+GENERATE_SQUARE)\n",
        "          # print(\"Cols \", c + GENERATE_SQUARE)\n",
        "          image_array[r:r+GENERATE_SQUARE,c:c+GENERATE_SQUARE] = generated_images[image_count] * 255\n",
        "          image_count += 1\n",
        "\n",
        "            \n",
        "    output_path = os.path.join(DATA_PATH,'output')\n",
        "    if not os.path.exists(output_path):\n",
        "      os.makedirs(output_path)\n",
        "    \n",
        "    filename = os.path.join(output_path,f\"train-{count}.png\")\n",
        "    im = Image.fromarray(image_array)\n",
        "    im.save(filename)\n",
        "  \n",
        "  def run(self, new_model, save_img_checkpoint, save_model_checkpoint):\n",
        "    if new_model:\n",
        "      #Build generator and discriminator \n",
        "      self.generator = self.build_generator()\n",
        "      self.discriminator = self.build_discriminator()\n",
        "    else:\n",
        "      #Load previous model\n",
        "      self.load_models()\n",
        "\n",
        "    #Compiling G and D\n",
        "    self.compile_models()\n",
        "\n",
        "    print('Starting training...')\n",
        "    self.train(self.training_dataset, self.training_labels, EPOCHS, save_img_checkpoint, save_model_checkpoint)\n",
        "\n",
        "    print('Saving generator model...')\n",
        "    self.save_models()\n",
        "\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81kOvi7se_gC"
      },
      "source": [
        "#### Running the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "u7pBBlQQfHqD",
        "outputId": "4d176206-1f67-40ed-f9e1-3f721ad391ef"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "  can = CAN()\n",
        "  can.run(new_model = True, save_img_checkpoint = 100, save_model_checkpoint = 500)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/4 [00:07<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['d_h8_lin/kernel:0', 'd_h8_lin/bias:0', 'd_h9_lin/kernel:0', 'd_h9_lin/bias:0', 'd_co_lin/kernel:0', 'd_co_lin/bias:0'] when minimizing the loss.\n",
            "tf.Tensor(115.97182, shape=(), dtype=float32)\n",
            "tf.Tensor(9.936165, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1d5659ca7caa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0mcan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCAN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mcan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_img_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_model_checkpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-59aa4b13e786>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, new_model, save_img_checkpoint, save_model_checkpoint)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Starting training...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_img_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_model_checkpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Saving generator model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-59aa4b13e786>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_dataset, training_labels, epochs, save_img_checkpoint, save_model_checkpoint)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;31m# show_image(real_images)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;31m# print(\"Running step\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_image_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0mg_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0md_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-59aa4b13e786>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, real_images, real_image_labels)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;31m#Calculate gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0mgen_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0mdis_gradients\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_tape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1086\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1088\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    604\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m           data_format=data_format)\n\u001b[0m\u001b[1;32m    607\u001b[0m   ]\n\u001b[1;32m    608\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_filter\u001b[0;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1082\u001b[0m         \u001b[0;34m\"strides\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"padding\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m         data_format, \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1085\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}